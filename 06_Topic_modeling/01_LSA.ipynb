{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# LSA : Latent Semantic Analysis\n",
    "\n",
    "LSA is not the algorithm for topic modeling but it provides idea for modeling.\n",
    "\n",
    "LDA is the algorithm fit for topic modeling, which enhanced the pit fall of LSA.\n",
    "\n",
    "## LSA\n",
    "\n",
    "LSA reduce dimension of TF-IDF or DTM matrix and reveal the latent meaning of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 9)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "np.shape(A)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Conduct full SVD\n",
    "\n",
    "U, s, VT = np.linalg.svd(A, full_matrices=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(4, 4)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check U Orthogonal matrix\n",
    "\n",
    "print(U.round(2))\n",
    "np.shape(U)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.69 2.05 1.73 0.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(4,)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s(sigma) singular value\n",
    "\n",
    "print(s.round(2))\n",
    "np.shape(s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(4, 9)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check D Orthogonal matrix\n",
    "\n",
    "S = np.zeros((4, 9))\n",
    "S[:4, :4] = np.diag(s)\n",
    "print(S.round(2))\n",
    "np.shape(S)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
     ]
    }
   ],
   "source": [
    "# Is the SVD successful?\n",
    "\n",
    "print(VT.round(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is decomposed data same with original?\n",
    "\n",
    "np.allclose(A, np.dot(np.dot(U,S), VT))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Truncated SVD\n",
    "\n",
    "SVD explaied above is called full SVD\n",
    "\n",
    "LSA uses truncated SVD. Truncated SVD deleted some vectors\n",
    "from U, s Vt matrices.\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/24949/svd%EC%99%80truncatedsvd.PNG)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 0.]\n",
      " [0. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Reduce dimension\n",
    "\n",
    "S = S[:2,:2]\n",
    "U = U[:, :2]\n",
    "VT = VT[:2, :]\n",
    "\n",
    "print(S.round())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.   -0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(A.round(2))\n",
    "print(np.dot(np.dot(U, S), VT).round(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example with Twenty Newsgroups\n",
    "\n",
    "Use LSA with Twenty Newsgroup in ScikitLearn.\n",
    "\n",
    "Should extract 5 main words from topics."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# It doesn't work ;(\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True,\n",
    "                             random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "<class 'list'>\n",
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(type(documents))\n",
    "print(documents[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "* remove special characters, use regex\n",
    "* remove short words\n",
    "* change alphabet to small letters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                document\n0      Well i'm not sure about the story nad it did s...\n1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n2      Although I realize that principle is not one o...\n3      Notwithstanding all the legitimate fuss about ...\n4      Well, I will have to change the scoring on my ...\n...                                                  ...\n11309  Danny Rubenstein, an Israeli journalist, will ...\n11310                                                 \\n\n11311  \\nI agree.  Home runs off Clemens are always m...\n11312  I used HP DeskJet with Orange Micros Grappler ...\n11313                                        ^^^^^^\\n...\n\n[11314 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Well i'm not sure about the story nad it did s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Although I realize that principle is not one o...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Notwithstanding all the legitimate fuss about ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Well, I will have to change the scoring on my ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11309</th>\n      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n    </tr>\n    <tr>\n      <th>11310</th>\n      <td>\\n</td>\n    </tr>\n    <tr>\n      <th>11311</th>\n      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n    </tr>\n    <tr>\n      <th>11312</th>\n      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n    </tr>\n    <tr>\n      <th>11313</th>\n      <td>^^^^^^\\n...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11314 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "news_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'       Yeah  do you expect people to read the FAQ  etc  and actually accept hard atheism   No  you need a little leap of faith  Jimmy   Your logic runs out of steam         Jim   Sorry I can t pity you  Jim   And I m sorry that you have these feelings of denial about the faith you need to get by   Oh well  just pretend that it will all end happily ever after anyway   Maybe if you start a new newsgroup  alt atheist hard  you won t be bummin  so much        Bye Bye  Big Jim   Don t forget your Flintstone s Chewables          Bake Timmons  III'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete special character\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "news_df.clean_doc[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'Yeah expect people read actually accept hard atheism need little leap faith Jimmy Your logic runs steam Sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway Maybe start newsgroup atheist hard bummin much forget your Flintstone Chewables Bake Timmons'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove short words\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "news_df['clean_doc'][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to small letters\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "news_df['clean_doc'][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kyuwon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
    "\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) # 불용어를 제거합니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_doc[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For TfidfVertorizer uses string data (not tokenized) so conduct detokendization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# 역토큰화 (토큰화 작업을 역으로 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenized_doc[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(11314, 1000)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "max_features= 1000, # 상위 1,000개의 단어를 보존\n",
    "max_df = 0.5,\n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape # TF-IDF 행렬의 크기 확인"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "(20, 1000)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1000)\n",
      "<class 'numpy.ndarray'>\n",
      "(20,)\n",
      "(20,)\n",
      "-0.0053570590833784725\n"
     ]
    }
   ],
   "source": [
    "print(svd_model.components_.shape)\n",
    "print(type(svd_model.components_))\n",
    "print(svd_model.singular_values_.shape)\n",
    "print(svd_model.explained_variance_.shape)\n",
    "print(svd_model.components_[1][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'able', 'accept', 'access', 'according', 'account', 'action', 'actions', 'actual', 'actually', 'added', 'addition', 'additional', 'address', 'administration', 'advance', 'advice', 'agencies', 'agree', 'algorithm', 'allow', 'allowed', 'allows', 'amendment', 'america', 'american', 'americans', 'analysis', 'angeles', 'anonymous', 'answer', 'answers', 'anti', 'anybody', 'apparently', 'appear', 'appears', 'apple', 'application', 'applications', 'apply', 'appreciate', 'appreciated', 'approach', 'appropriate', 'april', 'arab', 'archive', 'area', 'areas', 'argument', 'arguments', 'armenia', 'armenian', 'armenians', 'arms', 'army', 'article', 'articles', 'asked', 'asking', 'assume', 'assuming', 'atheism', 'atheists', 'attack', 'attempt', 'author', 'authority', 'available', 'average', 'avoid', 'away', 'background', 'base', 'baseball', 'based', 'basic', 'basically', 'basis', 'begin', 'beginning', 'belief', 'beliefs', 'believe', 'best', 'better', 'bible', 'bike', 'bios', 'bits', 'black', 'block', 'blood', 'blue', 'board', 'body', 'book', 'books', 'boston', 'bought', 'break', 'bring', 'brought', 'build', 'building', 'built', 'business', 'cable', 'california', 'called', 'calling', 'calls', 'came', 'canada', 'card', 'cards', 'care', 'carry', 'cars', 'case', 'cases', 'cause', 'center', 'certain', 'certainly', 'chance', 'change', 'changed', 'changes', 'char', 'character', 'cheap', 'check', 'chicago', 'child', 'children', 'chip', 'chips', 'choice', 'choose', 'christ', 'christian', 'christianity', 'christians', 'church', 'citizens', 'city', 'civil', 'claim', 'claims', 'class', 'clear', 'clearly', 'clinton', 'clipper', 'clock', 'close', 'code', 'color', 'colorado', 'colors', 'come', 'comes', 'coming', 'command', 'comment', 'comments', 'commercial', 'committee', 'common', 'communications', 'community', 'comp', 'companies', 'company', 'complete', 'completely', 'computer', 'conclusion', 'condition', 'conference', 'congress', 'connection', 'consider', 'considered', 'considering', 'contact', 'contains', 'context', 'continue', 'contrib', 'control', 'controller', 'convert', 'copies', 'copy', 'correct', 'cost', 'costs', 'count', 'countries', 'country', 'couple', 'course', 'court', 'cover', 'create', 'created', 'crime', 'criminals', 'cross', 'current', 'currently', 'data', 'date', 'dave', 'david', 'days', 'dead', 'deal', 'death', 'decided', 'decision', 'default', 'defense', 'define', 'deleted', 'department', 'described', 'description', 'design', 'designed', 'details', 'detroit', 'developed', 'development', 'device', 'devices', 'died', 'difference', 'different', 'difficult', 'digital', 'direct', 'directly', 'directory', 'discussion', 'disease', 'disk', 'disks', 'display', 'distribution', 'division', 'doctor', 'door', 'double', 'doubt', 'draw', 'drive', 'driver', 'drivers', 'drives', 'driving', 'drug', 'drugs', 'earlier', 'early', 'earth', 'easily', 'east', 'easy', 'education', 'effect', 'effective', 'effort', 'electronic', 'email', 'encryption', 'energy', 'enforcement', 'engine', 'entire', 'entry', 'environment', 'equipment', 'error', 'errors', 'escrow', 'especially', 'europe', 'event', 'events', 'evidence', 'exactly', 'example', 'excellent', 'exist', 'existence', 'exists', 'expect', 'expected', 'expensive', 'experience', 'explain', 'export', 'extra', 'face', 'fact', 'fairly', 'faith', 'fall', 'false', 'family', 'fast', 'faster', 'father', 'features', 'federal', 'feel', 'field', 'figure', 'file', 'files', 'final', 'finally', 'fine', 'firearms', 'floppy', 'folks', 'follow', 'following', 'font', 'fonts', 'food', 'force', 'forget', 'form', 'format', 'forward', 'free', 'freedom', 'friend', 'friends', 'fully', 'function', 'functions', 'future', 'game', 'games', 'gave', 'general', 'generally', 'genocide', 'germany', 'gets', 'getting', 'given', 'gives', 'giving', 'goal', 'goals', 'goes', 'going', 'gone', 'good', 'government', 'graphics', 'great', 'greatly', 'greek', 'ground', 'group', 'groups', 'guess', 'guns', 'guys', 'half', 'hand', 'handle', 'hands', 'happen', 'happened', 'happens', 'happy', 'hard', 'hardware', 'head', 'health', 'hear', 'heard', 'heaven', 'held', 'hell', 'hello', 'help', 'high', 'higher', 'history', 'hockey', 'hold', 'holy', 'home', 'hope', 'hours', 'house', 'human', 'idea', 'ideas', 'image', 'images', 'imagine', 'important', 'include', 'included', 'includes', 'including', 'increase', 'independent', 'individual', 'info', 'information', 'input', 'inside', 'installed', 'instead', 'institute', 'insurance', 'intended', 'interested', 'interesting', 'interface', 'internal', 'international', 'internet', 'involved', 'israel', 'israeli', 'issue', 'issues', 'james', 'jesus', 'jewish', 'jews', 'jobs', 'john', 'jpeg', 'kept', 'keyboard', 'keys', 'kill', 'killed', 'killing', 'kind', 'king', 'knew', 'know', 'knowledge', 'known', 'knows', 'lack', 'land', 'language', 'large', 'late', 'later', 'latest', 'launch', 'laws', 'lead', 'league', 'learn', 'leave', 'left', 'legal', 'letter', 'level', 'library', 'license', 'life', 'light', 'like', 'likely', 'limited', 'line', 'lines', 'list', 'little', 'live', 'lives', 'living', 'local', 'logic', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lord', 'lost', 'lots', 'love', 'lower', 'lunar', 'machine', 'machines', 'magazine', 'mail', 'mailing', 'main', 'major', 'majority', 'make', 'makes', 'making', 'manager', 'manual', 'march', 'mark', 'market', 'mass', 'master', 'material', 'matter', 'matthew', 'maybe', 'mean', 'meaning', 'means', 'media', 'medical', 'member', 'members', 'memory', 'mention', 'mentioned', 'message', 'messages', 'method', 'michael', 'middle', 'mike', 'miles', 'military', 'million', 'mind', 'minutes', 'misc', 'mission', 'mode', 'model', 'models', 'modem', 'modern', 'money', 'monitor', 'month', 'months', 'moon', 'moral', 'morning', 'mother', 'motif', 'mouse', 'multi', 'multiple', 'muslim', 'names', 'nasa', 'national', 'natural', 'nature', 'near', 'necessary', 'need', 'needed', 'needs', 'network', 'news', 'newsgroup', 'nice', 'night', 'normal', 'north', 'note', 'nuclear', 'null', 'number', 'numbers', 'object', 'objective', 'obvious', 'obviously', 'offer', 'offers', 'office', 'official', 'ones', 'open', 'operation', 'opinion', 'opinions', 'option', 'options', 'orbit', 'order', 'organization', 'original', 'output', 'outside', 'package', 'page', 'paid', 'pain', 'paper', 'particular', 'particularly', 'parts', 'party', 'pass', 'passed', 'past', 'patients', 'paul', 'peace', 'people', 'perfect', 'performance', 'period', 'person', 'personal', 'peter', 'phone', 'physical', 'pick', 'picture', 'pittsburgh', 'place', 'places', 'plan', 'play', 'played', 'player', 'players', 'playing', 'plus', 'point', 'points', 'police', 'policy', 'political', 'poor', 'population', 'port', 'position', 'possible', 'possibly', 'post', 'posted', 'posting', 'posts', 'postscript', 'power', 'practice', 'present', 'president', 'press', 'pretty', 'prevent', 'previous', 'price', 'print', 'printer', 'privacy', 'private', 'probably', 'problem', 'problems', 'process', 'processing', 'produce', 'product', 'products', 'program', 'programming', 'programs', 'project', 'property', 'protect', 'protection', 'prove', 'provide', 'provided', 'provides', 'public', 'published', 'purpose', 'quality', 'question', 'questions', 'quite', 'quote', 'radio', 'range', 'rate', 'rates', 'read', 'reading', 'real', 'reality', 'realize', 'really', 'reason', 'reasonable', 'reasons', 'receive', 'received', 'recent', 'recently', 'record', 'reference', 'references', 'regarding', 'regular', 'related', 'release', 'religion', 'religious', 'remember', 'remote', 'reply', 'report', 'reported', 'reports', 'request', 'require', 'required', 'requires', 'research', 'resource', 'resources', 'respect', 'response', 'rest', 'result', 'results', 'return', 'right', 'rights', 'risk', 'road', 'robert', 'room', 'round', 'rule', 'rules', 'running', 'runs', 'russia', 'russian', 'safe', 'safety', 'said', 'sale', 'satellite', 'save', 'saying', 'says', 'school', 'science', 'scientific', 'screen', 'scsi', 'search', 'season', 'second', 'secret', 'section', 'secure', 'security', 'seen', 'self', 'sell', 'send', 'sense', 'sent', 'serial', 'series', 'seriously', 'server', 'service', 'services', 'setting', 'shall', 'shipping', 'short', 'shot', 'shows', 'shuttle', 'similar', 'simple', 'simply', 'single', 'site', 'sites', 'situation', 'size', 'slow', 'small', 'smith', 'social', 'society', 'software', 'sold', 'soldiers', 'solution', 'somebody', 'soon', 'sorry', 'sort', 'sound', 'sounds', 'source', 'sources', 'south', 'soviet', 'space', 'speak', 'special', 'specific', 'specifically', 'speed', 'spirit', 'stand', 'standard', 'standards', 'start', 'started', 'starting', 'state', 'stated', 'statement', 'states', 'station', 'stay', 'step', 'stephanopoulos', 'steve', 'stop', 'story', 'stream', 'street', 'strong', 'studies', 'study', 'stuff', 'stupid', 'subject', 'suggest', 'suggestions', 'summer', 'supply', 'support', 'supported', 'supports', 'suppose', 'supposed', 'sure', 'surface', 'suspect', 'switch', 'systems', 'table', 'taken', 'takes', 'taking', 'talk', 'talking', 'tape', 'team', 'teams', 'technical', 'technology', 'tell', 'term', 'terms', 'test', 'text', 'thank', 'thanks', 'theory', 'thing', 'things', 'think', 'thinking', 'thought', 'time', 'times', 'title', 'today', 'told', 'took', 'tools', 'toronto', 'total', 'town', 'trade', 'traffic', 'transfer', 'tried', 'trouble', 'true', 'trust', 'truth', 'trying', 'turkey', 'turkish', 'turks', 'turn', 'turned', 'type', 'types', 'understand', 'understanding', 'unfortunately', 'unit', 'united', 'universe', 'university', 'unix', 'unless', 'used', 'useful', 'usenet', 'user', 'users', 'uses', 'using', 'usually', 'valid', 'value', 'values', 'vancouver', 'various', 'vehicle', 'version', 'versions', 'video', 'view', 'voice', 'volume', 'wait', 'want', 'wanted', 'wants', 'washington', 'watch', 'water', 'ways', 'weapon', 'weapons', 'week', 'weeks', 'went', 'west', 'western', 'white', 'wide', 'widget', 'wife', 'willing', 'window', 'windows', 'wire', 'wish', 'woman', 'women', 'wonder', 'wondering', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worse', 'worth', 'write', 'writing', 'written', 'wrong', 'wrote', 'xterm', 'yeah', 'year', 'years', 'york', 'young']\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer analyzed whole words and got most frequently appearing 1000 words\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(terms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic 2 [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
      "Topic 3 [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
      "Topic 4 [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
      "Topic 5 [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
      "Topic 6 [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
      "Topic 7 [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
      "Topic 8 [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
      "Topic 9 [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
      "Topic 10 [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
      "Topic 11 [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
      "Topic 12 [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
      "Topic 13 [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
      "Topic 14 [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
      "Topic 15 [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
      "Topic 16 [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
      "Topic 17 [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
      "Topic 18 [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
      "Topic 19 [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
      "Topic 20 [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
     ]
    }
   ],
   "source": [
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print('Topic %d' % (idx+1),\n",
    "              [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n -1: -1]])\n",
    "\n",
    "get_topics(svd_model.components_,terms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pros and Cons of LSA\n",
    "\n",
    "* Pros\n",
    "    * Easy to implement\n",
    "    * Great performance for similarity calculation\n",
    "* Cons\n",
    "    * Hard to update with new data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}